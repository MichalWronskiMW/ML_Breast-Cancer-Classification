


import numpy as np
import pandas as pd

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

import sys
sys.path.append('../functions')
from functions import evaluate_classifier
import pickle


# Load scaled train/test data
with open("../data/02_train-test-data-scaled.pkl", "rb") as f:
    X_train_scaled, X_test_scaled, y_train, y_test = pickle.load(f)

# Load raw (unscaled) train/test data
with open("../data/02_train-test-data-raw.pkl", "rb") as f:
    X_train, X_test, y_train, y_test = pickle.load(f)


X_train_scaled





model_results = pd.DataFrame({
    "Model": pd.Series(dtype='str'),
    "Train Accuracy": pd.Series(dtype='float'),
    "Test Accuracy": pd.Series(dtype='float'),
    "Train Precision": pd.Series(dtype='float'),
    "Test Precision": pd.Series(dtype='float'),
    "Train Recall": pd.Series(dtype='float'),
    "Test Recall": pd.Series(dtype='float'),
    "Train F1": pd.Series(dtype='float'),
    "Test F1": pd.Series(dtype='float'),
})





lg = LogisticRegression(max_iter=5000)
lg.fit(X_train_scaled, y_train)


evaluate_classifier(lg, "Logistic Regression", X_train_scaled, y_train, X_test_scaled, y_test)





knn = KNeighborsClassifier(n_neighbors=21)
knn.fit(X_train_scaled, y_train)


evaluate_classifier(knn, "KNN (k=21)", X_train_scaled, y_train, X_test_scaled, y_test, average='macro')





dt = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=5,
    min_samples_leaf=1,
    random_state=4)

dt.fit(X_train_scaled, y_train)


evaluate_classifier(dt, "Decision Tree", X_train_scaled, y_train, X_test_scaled, y_test, average='macro')





rf = RandomForestClassifier(max_depth=5, random_state=4)
rf.fit(X_train_scaled, y_train)


evaluate_classifier(rf, "Random Forest", X_train_scaled, y_train, X_test_scaled, y_test, average='macro')





xgb = XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    eval_metric='logloss',
    random_state=42
)


xgb.fit(X_train_scaled, y_train)


evaluate_classifier(xgb, "XGBoost", X_train_scaled, y_train, X_test_scaled, y_test, average='macro')


model_results



